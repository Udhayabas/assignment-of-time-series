{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc54ad2",
   "metadata": {},
   "source": [
    "# explanation of comintion function of VAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a909213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.function defined with dataset and listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cominbation(dataset,listt):\n",
    "    print(listt)\n",
    "    datasetTwo=dataset[listt]\n",
    "    test_obs = 28\n",
    "    train =datasetTwo[:-test_obs]\n",
    "    test = datasetTwo[-test_obs:]\n",
    "    from statsmodels.tsa.api import VAR\n",
    "    for i in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        model = VAR(train)\n",
    "        results = model.fit(i)\n",
    "        print('Order =', i)\n",
    "        print('AIC: ', results.aic)\n",
    "        print('BIC: ', results.bic)\n",
    "        print()\n",
    "    x = model.select_order(maxlags=12)\n",
    "    order=x.selected_orders[\"aic\"]\n",
    "    result = model.fit(order)\n",
    "    #result.summary()\n",
    "    lagged_Values = train.values[-order:]\n",
    "    pred = result.forecast(y=lagged_Values,steps=28) \n",
    "    preds=pd.DataFrame(pred,columns=listt)\n",
    "    preds.to_csv(\"varforecasted_{}.csv\".format(test_obs))\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    rmse= round(mean_squared_error(test,pred,squared=False))\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "    mape=mean_absolute_percentage_error(test,pred)\n",
    "    performance[\"Model\"].append(listt)\n",
    "    performance[\"RMSE\"].append(rmse)\n",
    "    performance[\"MaPe\"].append(mape)\n",
    "    performance[\"Lag\"].append(order)\n",
    "    performance[\"Test\"].append(test_obs)\n",
    "    perf=pd.DataFrame(performance)\n",
    "    return perf,result,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.function defined with dataset and listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95887aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cominbation(dataset, listt):\n",
    "    print(listt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.splitting dataset with last28 is testsize and remaining is train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c880fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8028\\3839052107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatasetTwo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlistt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetTwo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtest_obs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetTwo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtest_obs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "    datasetTwo = dataset[listt]\n",
    "    test_obs = 28\n",
    "    train = datasetTwo[:-test_obs]\n",
    "    test = datasetTwo[-test_obs:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAR model is import from the statsmodel\n",
    "#forloop for lagorder 1-10\n",
    "#var model fitted with training set for creating model\n",
    "#The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "#for the model var model is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc060a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aic is a measure of different models of statistical model and choose best model\n",
    "#bic is also a same but  it analyse different parameter to choos best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30827110",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from statsmodels.tsa.api import VAR\n",
    "    for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        model = VAR(train)\n",
    "        results = model.fit(i)\n",
    "        print('Order =', i)\n",
    "        print('AIC: ', results.aic)\n",
    "        print('BIC: ', results.bic)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222620a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to choose best optimal lag order for aic model with maximum 12 lag\n",
    "#created best model fitted with optimal lag order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = model.select_order(maxlags=12)\n",
    "    order = x.selected_orders[\"aic\"]\n",
    "    result = model.fit(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74554258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lagged values from training set is strating point of forecasting values\n",
    "#forecast next 28 from the created modedel\n",
    "#forecasted valuse converted into dataframe and saved to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612af2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    lagged_Values = train.values[-order:]\n",
    "    pred = result.forecast(y=lagged_Values, steps=28)\n",
    "    preds = pd.DataFrame(pred, columns=listt)\n",
    "    preds.to_csv(\"varforecasted_{}.csv\".format(test_obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE) are imported.\n",
    "#RMSE and MAPE are calculated from the test set and the forecasted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "    rmse = round(mean_squared_error(test, pred, squared=False))\n",
    "    mape = mean_absolute_percentage_error(test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance metrics and model details are appended to performance dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "    performance[\"Model\"].append(listt)\n",
    "    performance[\"RMSE\"].append(rmse)\n",
    "    performance[\"MaPe\"].append(mape)\n",
    "    performance[\"Lag\"].append(order)\n",
    "    performance[\"Test\"].append(test_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance dictionary format is converted into dataframe\n",
    "#function return performance dictionary,model,forcasted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "    perf = pd.DataFrame(performance)\n",
    "    return perf, result, pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
